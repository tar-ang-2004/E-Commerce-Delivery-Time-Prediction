version: '3.8'

services:
  deliveryai:
    build: .
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - MLFLOW_TRACKING_URI=file:./mlruns
    volumes:
      - ./models:/app/models
      - ./mlruns:/app/mlruns
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - deliveryai-network

  mlflow:
    image: python:3.9-slim
    command: >
      bash -c "pip install mlflow && 
               mlflow server 
               --backend-store-uri file:///mlflow/mlruns 
               --default-artifact-root file:///mlflow/mlartifacts 
               --host 0.0.0.0 
               --port 5001"
    ports:
      - "5001:5001"
    volumes:
      - ./mlruns:/mlflow/mlruns
      - ./mlartifacts:/mlflow/mlartifacts
    restart: unless-stopped
    networks:
      - deliveryai-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - deliveryai
      - mlflow
    restart: unless-stopped
    networks:
      - deliveryai-network

networks:
  deliveryai-network:
    driver: bridge

volumes:
  mlflow-data:
  model-data: